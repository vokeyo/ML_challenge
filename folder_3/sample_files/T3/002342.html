
<HTML>
<HEAD>
<STYLE TYPE=text/css>
.change {background-color: white; color: black}
</STYLE>

<title>
NERSC FY 1999 User Survey Results: Hardware Resources 
</title>

<!-- The following "exec" server side include generates the
remaining common fields of the header and begins the body -->

<link rel="stylesheet" type="text/css" media="screen,print" href="/css/main.css" />
<link rel="stylesheet" type="text/css" media="screen,print" href="/css/nav.css" />
<link rel="stylesheet" type="text/css" media="screen,print" href="/css/categories.css" />
<link rel="alternate stylesheet" type="text/css" media="screen" href="/css/main_large.css" title="Large Text"/>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"></meta>
</head>
<body>


<!-- Begin HTML page content following this line -->
<CENTER>
<H1>NERSC FY 1999 User Survey Results: Hardware Resources</H1>
<H4>Legend</H4>
<TABLE BORDER=1 BGCOLOR=steelblue>
<TR><TH><FONT COLOR=WHITE>Satisfaction</FONT></TH><TH><FONT COLOR=WHITE>Value</FONT></TH></TR>
<TR BGCOLOR=#EEEEEE><TD>Very Satisfied</TD><TD ALIGN=CENTER>7</TD></TR>
<TR BGCOLOR=#CCCCCC><TD>Mostly Satisfied</TD><TD ALIGN=CENTER>6</TD></TR>
<TR BGCOLOR=#AAAAAA><TD>Somewhat Satisfied</TD><TD ALIGN=CENTER>5</TD></TR>
<TR BGCOLOR=#888888><TD><FONT COLOR=WHITE>Neutral</FONT></TD><TD ALIGN=CENTER><FONT COLOR=WHITE>4</FONT></TD></TR>
<TR BGCOLOR=#666666><TD><FONT COLOR=WHITE>Somewhat Dissatisfied</FONT></TD><TD ALIGN=CENTER><FONT COLOR=WHITE>3</FONT></TD></TR>
<TR BGCOLOR=#444444><TD><FONT COLOR=WHITE>Mostly Dissatisfied</FONT></TD><TD ALIGN=CENTER><FONT COLOR=WHITE>2</FONT></TD></TR>
<TR BGCOLOR=#222222><TD><FONT COLOR=WHITE>Very Dissatisfied</FONT></TD><TD ALIGN=CENTER><FONT COLOR=WHITE>1</FONT></TD></TR>

</TABLE>
<P>
<H2>Cray T3E - MCurie</H2>
<TABLE BORDER=1 BGCOLOR=lightsteelblue>
<TR BGCOLOR=STEELBLUE><TH ROWSPAN=2><FONT COLOR=WHITE>Topic</FONT></TH>
<TH COLSPAN=3><FONT COLOR=WHITE>Satisfaction</FONT></TH>
</TR>
<TR BGCOLOR=steelblue><TH><FONT COLOR=WHITE>No. of Responses</FONT></TH><TH><FONT COLOR=WHITE>Avg.</FONT></TH>
<TH><FONT COLOR=WHITE>'98/Change</TH>
</TR>


<TR BGCOLOR=#CCCCCC> <TD>Uptime</TD>
<TD ALIGN=CENTER>93</TD> <TD ALIGN=CENTER>6.26</TD>
<TD CLASS=change>5.58/+0.68</TD></TR>

<TR BGCOLOR=#CCCCCC> <TD>Overall</TD>
<TD ALIGN=CENTER>64</TD> <TD ALIGN=CENTER>6.17</TD>
<TD CLASS=change>5.20/+0.97</TD></TR>

<TR BGCOLOR=#CCCCCC> <TD>Ability to run interactively</TD>
<TD ALIGN=CENTER>85</TD> <TD ALIGN=CENTER>5.60</TD>
<TD CLASS=change>&nbsp;</TD></TR>

<TR BGCOLOR=#AAAAAA> <TD>Batch queue structure</TD>
<TD ALIGN=CENTER>81</TD> <TD ALIGN=CENTER>5.47</TD>
<TD CLASS=change>4.51/+0.96</TD></TR>

<TR BGCOLOR=#AAAAAA> <TD>Disk configuration and I/O performance</TD>
<TD ALIGN=CENTER>71</TD> <TD ALIGN=CENTER>5.23</TD>
<TD CLASS=change>&nbsp;</TD></TR>

<TR BGCOLOR=#AAAAAA> <TD>Batch job wait time</TD>
<TD ALIGN=CENTER>80</TD> <TD ALIGN=CENTER>5.04</TD>
<TD CLASS=change>4.43/+0.61</TD></TR>
</TABLE><P>

<P><TABLE BORDER=1 BGCOLOR=lightsteelblue>
<TR><TH>Question</TH><TH>No. of Responses</TH><TH>Avg.</TH></TR>
<TR> <TD>Uptime estimate (%)</TD>
<TD ALIGN=CENTER>56</TD> <TD ALIGN=CENTER>
89.6</TD></TR>
<TR> <TD>Batch wait time estimate (hours)</TD>
<TD ALIGN=CENTER>49</TD> <TD ALIGN=CENTER>
14.5</TD></TR>
<TR> <TD>Max. number of PEs used</TD>
<TD ALIGN=CENTER>77</TD> <TD ALIGN=CENTER>
142.2</TD></TR>
<TR> <TD>Max. number of PEs code can effectively use</TD>
<TD ALIGN=CENTER>60</TD> <TD ALIGN=CENTER>
379.4</TD></TR>
</TABLE><P>

<H2>Cray PVP Cluster</H2>
<TABLE BORDER=1 BGCOLOR=lightsteelblue>
<TR BGCOLOR=STEELBLUE><TH ROWSPAN=2><FONT COLOR=WHITE>Topic</FONT></TH>
<TH COLSPAN=3><FONT COLOR=WHITE>Satisfaction</FONT></TH></TR>
<TR BGCOLOR=steelblue><TH><FONT COLOR=WHITE>No. of Responses</FONT></TH>
<TH><FONT COLOR=WHITE>Avg.</FONT></TH><TH><FONT COLOR=WHITE>'98/Change</FONT></TH></TR>


<TR BGCOLOR=#CCCCCC> <TD>Uptime</TD>
<TD ALIGN=CENTER>73</TD> <TD ALIGN=CENTER>6.29</TD>
<TD CLASS=change>5.69/+0.60</TD></TR>

<TR BGCOLOR=#CCCCCC> <TD>Disk configuration and I/O performance</TD>
<TD ALIGN=CENTER>54</TD> <TD ALIGN=CENTER>5.56</TD>
<TD CLASS=change>&nbsp;</TD></TR>

<TR BGCOLOR=#AAAAAA> <TD>Ability to run interactively</TD>
<TD ALIGN=CENTER>68</TD> <TD ALIGN=CENTER>5.18</TD>
<TD CLASS=change>&nbsp;</TD></TR>

<TR BGCOLOR=#AAAAAA> <TD>Overall</TD>
<TD ALIGN=CENTER>58</TD> <TD ALIGN=CENTER>5.05</TD>
<TD CLASS=change>4.92/+0.13</TD></TR>

<TR BGCOLOR=#AAAAAA> <TD>Batch queue structure</TD>
<TD ALIGN=CENTER>60</TD> <TD ALIGN=CENTER>5.03</TD>
<TD CLASS=change>4.85/+0.18</TD></TR>

<TR BGCOLOR=#888888> <TD><FONT COLOR=WHITE>Batch job wait time</FONT></TD>
<TD ALIGN=CENTER><FONT COLOR=WHITE>62</FONT></TD> <TD ALIGN=CENTER><FONT COLOR=WHITE>3.95</FONT></TD>
<TD CLASS=change>4.79/-0.84</TD></TR>


</TABLE><P>
<P><TABLE BORDER=1 BGCOLOR=lightsteelblue>
<TR><TH>Question</TH><TH>No. of Responses</TH><TH>Avg.</TH></TR>
<TR> <TD>Uptime estimate (%)</TD>
<TD ALIGN=CENTER>48</TD> <TD ALIGN=CENTER>
88.6</TD></TR>
<TR> <TD>Batch wait time estimate (hours)</TD>
<TD ALIGN=CENTER>43</TD> <TD ALIGN=CENTER>
43.7</TD></TR>
</TABLE><P>

<H2>HPSS</H2>
<TABLE BORDER=1 BGCOLOR=lightsteelblue>
<TR BGCOLOR=STEELBLUE><TH ROWSPAN=2><FONT COLOR=WHITE>Topic</FONT></TH>
<TH COLSPAN=3><FONT COLOR=WHITE>Satisfaction</FONT></TH></TR>
<TR BGCOLOR=steelblue><TH><FONT COLOR=WHITE>No. of Responses</FONT></TH>
<TH><FONT COLOR=WHITE>Avg.</FONT></TH><TH><FONT COLOR=WHITE>'98/Change</FONT></TH></TR>

<TR BGCOLOR=#CCCCCC> <TD>Reliability</TD>
<TD ALIGN=CENTER>81</TD> <TD ALIGN=CENTER>6.46</TD>
<TD CLASS=change>5.51/+0.95</TD></TR>

<TR BGCOLOR=#CCCCCC> <TD>Uptime</TD>
<TD ALIGN=CENTER>81</TD> <TD ALIGN=CENTER>6.33</TD>
<TD CLASS=change>5.39/+0.94</TD></TR>

<TR BGCOLOR=#CCCCCC> <TD>User Interface</TD>
<TD ALIGN=CENTER>72</TD> <TD ALIGN=CENTER>6.06</TD>
<TD CLASS=change>4.88/+1.18</TD></TR>

<TR BGCOLOR=#CCCCCC> <TD>Overall</TD>
<TD ALIGN=CENTER>69</TD> <TD ALIGN=CENTER>6.12</TD>
<TD CLASS=change>5.09/+1.03</TD></TR>

<TR BGCOLOR=#CCCCCC> <TD>Performance</TD>
<TD ALIGN=CENTER>73</TD> <TD ALIGN=CENTER>5.90</TD>
<TD CLASS=change>5.46/+0.44</TD></TR>

<TR BGCOLOR=#CCCCCC> <TD>Response Time</TD>
<TD ALIGN=CENTER>75</TD> <TD ALIGN=CENTER>5.68</TD>
<TD CLASS=change>5.29/+0.39</TD></TR>

</TABLE><P>
<P><TABLE BORDER=1 BGCOLOR=lightsteelblue>
<TR><TH>Question</TH><TH>No. of Responses</TH><TH>Avg.</TH></TR>
<TR> <TD>Uptime estimate (%)</TD>
<TD ALIGN=CENTER>50</TD> <TD ALIGN=CENTER>
91.7</TD></TR>
<TR> <TD>Reliability estimate (%)</TD>
<TD ALIGN=CENTER>45</TD> <TD ALIGN=CENTER>
94.2</TD></TR>
<TR> <TD>Performance estimate (MB/sec)</TD>
<TD ALIGN=CENTER>12</TD> <TD ALIGN=CENTER>
11.8</TD></TR>
</TABLE><P>

<H2>Servers</H2>
<TABLE BORDER=1 BGCOLOR=lightsteelblue>
<TR BGCOLOR=STEELBLUE><TH ROWSPAN=2><FONT COLOR=WHITE>Topic</FONT></TH><TH COLSPAN=2><FONT COLOR=WHITE>Satisfaction</FONT></TH></TR>
<TR BGCOLOR=steelblue><TH><FONT COLOR=WHITE>No. of Responses</FONT></TH><TH><FONT COLOR=WHITE>Avg.</FONT></TH></TR>

<TR BGCOLOR=#AAAAAA> <TD>Visualization Server - Escher</TD>
<TD ALIGN=CENTER>11</TD> <TD ALIGN=CENTER>5.45</TD></TR>

<TR BGCOLOR=#AAAAAA> <TD>Math Server - Newton</TD>
<TD ALIGN=CENTER>12</TD> <TD ALIGN=CENTER>5.25</TD></TR>

</TABLE><P>
</CENTER>
<P>
<h2>Comments on NERSC's Cray T3E - 39 responses</h2>

<ul>
<li>&nbsp; 7: &nbsp; &nbsp;  <a href=#nice>A good machine / get more T3E
    processors</a>
    <ul><ul>
    <li>Stable, reliable. 
    <li>Good scalability. 
    <li>Provides lots of production time.
    </ul></ul>
<li>&nbsp; 7: &nbsp; &nbsp;  <a href=#queues>Comments on queue structure</a>
    <ul><ul>
    <li>7 asked for longer time limits.
    <li>1 asked for fairer turnaround across queues.
    </ul></ul>
<li>&nbsp; 6: &nbsp; &nbsp;  <a href=#thru>Comments on throughput,
    performance</a>
    <ul><ul>
    <li>4 said bad; 2 said good.
    </ul></ul>
<li>&nbsp; 6: &nbsp; &nbsp;  <a href=#inter>Comments on  interactive 
    computing</a>
    <ul><ul>
    <li>4 asked for more; 2 said good.
    </ul></ul>
<li>&nbsp; 5: &nbsp; &nbsp;  <a href=#disks>Comments on disks, inodes, file
    systems</a>
    <ul><ul>
    <li>4 said inodes too restricted.
    <li>1 said /usr/tmp too full.
    </ul></ul>
<li>&nbsp; 2: &nbsp; &nbsp;  <a href=#memory>Comments on memory</a>
    <ul><ul>
    <li>1 said enough; 1 said need more.
    </ul></ul>
<li>&nbsp; 5: &nbsp; &nbsp;  <a href=#othert3e>Other comments</a>
    <ul><ul>
    <li>need more optimization info.
    <li>Provide better queue stats (let us know which queue best to use now).
    <li>PVM errors, I/O bottlenecks.
    </ul></ul>
<li>&nbsp; 7: &nbsp; &nbsp;  <a href=#dkt3e>Don't use it (yet) / no specific
    comments</a>
</ul>

<h2>Did the C90 retirement go as smoothly as possible? - 42 responses</h2>

<ul>
<li>31: &nbsp; &nbsp;  <a href=#yes>Yes</a>
<li>&nbsp; 2: &nbsp; &nbsp;  <a href=#yesbut>Yes, but...</a>
    <ul><ul>
    <li>Would have liked access to C90 files right away.
    <li>Too many changes at the same time.
    </ul></ul>
<li>&nbsp; 5: &nbsp; &nbsp;  <a href=#no>No</a>
    <ul><ul>
    <li>Too much congestion on the J90s afterwards.
    <li>We lost cf77 and DISSPLA.
    <li>Now there is less interactive access.
    <li>Should not have removed C90 until full SV1 cluster was in place.
    <li>"As smoothly as possible" should have meant keeping the C90!
    </ul></ul>
<li>&nbsp; 4: &nbsp; &nbsp;  <a href=#nouse>Didn't use the C90 / hadn't used it
    recently</a>
</ul>

<h2>Does the current NERSC PVP cluster meet your vector computing needs?
      - 52 responses</h2>

<ul>
<li>24: &nbsp; &nbsp;  <a href=#nopvp>No</a>
<li>&nbsp; 3: &nbsp; &nbsp;  <a href=#probno>Probably no / am waiting to see</a>
<li>&nbsp; 3: &nbsp; &nbsp;  <a href=#yesbutpvp>Yes, but... / yes and no</a>
    <ul><ul>
    <li>Slow throughput / slow processors: 20 responses.
    <li>Need interactive SV1s / interactive too slow: 8 responses.
    <li>Shouldn't interactive environment be same as batch environment?
    <li>Doesn't like charging rate.
    <li>Problems with multitasking (OpenMP) and C++.
    <li>Disk system poorly configured.
    <li>Poor network access.
    <li>cqstatl response slow
    <li>Wants prompt to use the arrow keys.
    <li>Doubts he can run BASIS.
    </ul></ul>
<li>21: &nbsp; &nbsp;  <a href=#yespvp>Yes</a>
<li>&nbsp; 1: &nbsp; &nbsp;  <a href=#napvp>No answer</a>
</ul>

<h2>Comments on NERSC's Cray PVP Cluster - 26 responses</h2>

<ul>
<li>15: &nbsp; &nbsp;  <a href=#slow>Slow throughput, slow processors, 
    need more PVP cycles</a>
<li>&nbsp; 5: &nbsp; &nbsp; <a href=#needint>Need interactive SV1s, 
   interactive too slow</a>
<li>&nbsp; 4: &nbsp; &nbsp; <a href=#pvpsoft>Comments on software, 
    configuration</a>
    <ul><ul>
    <li>Doesn't like the automatic logout on killeen.
    <li>Doesn't like f90 compiler.
    <li>Multitasking (OpenMP) instabilities.
    <li>Disk system poorly configured.
    <li>Needs more info on text editors.
    </ul></ul>
<li>&nbsp; 4: &nbsp; &nbsp; <a href=#pvpgood>They're good machines, 
    satisfied</a>
    <ul><ul>
    <li>Stable, good software development/testing environment.
    <li>Excellent accuracy.
    <li>Running short jobs works well.
    </ul></ul>
<li>&nbsp; 3: &nbsp; &nbsp; <a href=#pvpq>Comments on queues</a>
    <ul><ul>
    <li>Needs a queue with 200 MW memory, 50 GB disk, 100-150 hours.
    <li>Make scheduling fairer.
    <li>Too many jobs are allowed for 1 user.
    <li>Too many job failures.
    <li>Frustrated when exceeds CPU and disk limits. 
    </ul></ul>
<li>&nbsp; 1: &nbsp; &nbsp; <a href=#pvpnot>Haven't used them</a>
</ul>

<h2>Comments on NERSC's HPSS Storage System - 38 responses</h2>

<ul>
<li>12: &nbsp; &nbsp; <a href=#likehpss>It's a good system, satisfied</a>
    <ul><ul>
    <li>Can easily handle large (8+GB) files.
    <li>Happy with reliability.
    </ul></ul>
<li>11: &nbsp; &nbsp; <a href=#interface>Would like a better user interface</a>
    <ul><ul>
    <li>More like Unix, more like NFS (3 responses).
    <li>Provide the ability to manipulate entire directories (3 responses).
    <li>Provide better failure recovery (2 responses).
    <li>Provide better search capabilities (2 responses).
    </ul></ul>
<li>&nbsp; 7: &nbsp; &nbsp; <a href=#xfer>Improve the transfer rate</a>
<li>&nbsp; 4: &nbsp; &nbsp; <a href=#cmd>Command response too slow</a>
<li>&nbsp; 3: &nbsp; &nbsp; <a href=#reliability>Stability problems</a>
<li>&nbsp; 2: &nbsp; &nbsp; <a href=#pswd>Password problems</a>
<li>&nbsp; 4: &nbsp; &nbsp; <a href=#dkhpss>Don't know what it is, don't 
     use</a>:  
</ul>

<h2>Comments about NERSC's auxiliary servers - 10 responses</h2>

<ul>
<li>&nbsp; 2: &nbsp; &nbsp; <a href=#likeaux>Like them</a>
<li>&nbsp; 1: &nbsp; &nbsp; <a href=#newton>Newton needs an upgrade</a>
<li>&nbsp; 1: &nbsp; &nbsp; <a href=#newweb>Provide a data broadcasting web 
       server</a>
<li>&nbsp; 6: &nbsp; &nbsp; <a href=#dkaux>Don't use them, no answer</a>
</ul>

<hr noshade>
<h4>Comments on NERSC's Cray T3E - 39 responses</h4>

<ul>
<li><b><a name=nice>A good machine / get more T3E processors</a>: 
     &nbsp; 7 responses</b>
<p>
the best supercomputer I know of 
<p>
Nice machine ! 
<p>
It is definitely a very stable and reliable machine!!! 
<p>
I like the machine, and it is helpful to my work. 
<p>
Ideally, it would be good to have more T3E's so that more projects could be 
awarded greater than 10**5 processor hrs./year.
<p>
Good scalability. [...]
<p>
The machine is extremely useful in the amount of production time that is 
available on it, however it is out of date as a massively parallel 
supercomputer.

<p>
<li><b><a name=queues>Comments on queue structure</a>: &nbsp; 7 responses</b>
<p>
I'd like to have a long 512-processor queue 
<p>
Batch queues that run longer than 4 hours would be nice.  12 hours would make
many of my jobs easier to run and manage.  Some things just can't be done in
4 hour segments. 
<p>
Some jobs in the production queue start fairly quickly.  The pe128 especially
seems to be under utilized.   But for long jobs, the wait can the long.  In
one case I launched a job on gc128 and was disappointed by the slow turn
around.  The jobs started the evening after I submitted, then ran a couple of
hours for the next two nights.  The results were not ready for 3 days.  I
would have been better off, I think, if I submitted multiple jobs in the
production queues.   <br> I've never tried more than 256 processors, but I
could.  I have some problems that I would like to solve that are too large
for the current queue configurations.  I am confident that my code will scale
past 256. 
<p>
Time limits per job seem to be too restrictive for long jobs, but that
require few processors. For example, the T3E system allows a time limit
of 4:10 h for jobs with less that 64 processors. 
<p>
Climate modeling requires very long runs. In general, the queue structure
is inappropriate for these multi-month calculations.
<p>
As a "naive" user, I do not know what the possibilities or alternatives 
might be with regard to batch turn-around time. I would like to be able
to run batch jobs longer than 4 hours with fewer than 64 processor
<p>
It would be good to be able to run a job for longer time, say 24 hours, 
maybe with smaller number of processors (4-64). 

<p>
<li><b><a name=thru>Comments on throughput, performance</a>: 
    &nbsp; 6 responses</b>
<p>
Recently the queue wait time has been a real problem. 
<p>
The batch job wait time is too slow 
<p>
Too more users on MCURIE 
<p>
[...]Batch jobs are sometimes slow getting into and through the
queues.  This tends to lead to heavy interactive use to get adequate response
time.  My jobs tend to grow in time and thus require increasing numbers of
processors.  This is difficult to manage with slow queues.
<p>
good turnaround; few down times
<p>
Very high throughput compensates for lack of speed. [...]

<p>
<li><b><a name=inter>Comments on interactive computing</a>: 
    &nbsp; 6 responses</b>
<p>
Interactive and network performance are poor.  [...]
<p>
I asked my postdoc.  His main complaint was with the inability to run
small jobs interactively when two big jobs were doing so.  Maybe some 
sort of queue where a new job can take at most half of the available
processors. 
<p>
I know it's very hard to set up this architecture to run with multiple jobs
like a PVP --- but some interactive use of many processors would help
debugging.
<p>
If possible, keep a small number of processors available for debugging 
after midnight.
<p>
I find the ability to run parallel jobs interactively is particularly useful
as I am developing a parallel code and this makes debugging a lot easier
compared with some other systems that I use. 
<p>
Do a lot of my work on your T3E because I can run small test
interactively, as well as have access to totalview.  This is probably THE
most important thing for me.  Do not like it when mcurie is not available
after 4 pm on some weekdays, but can live with it.  Don't run  batch jobs
very often, other people in the group do that. 

<p>
<li><b><a name=disks>Comments on disks, inodes, file systems</a>: &nbsp; 
     5 responses</b>
<p>
[...] The
/usr/tmp/ disks are lately full enough that one has to take special measures
with large datasets to avoid having a PE hang in an I/O cycle.  Richard
Gerber helped me with this problem, and knows what I am talking about.<br>
The NERSC implementation of AFS is not user friendly.  I have given up on it,
and find this annoying.  I never figured out how (from a NERSC computer) to
access AFS directories that belonged to a different user -- sharing code
development directories with other users is one of the main reasons I use(d)
AFS.
<p>
[...] The limitation on the
number of files allowed in the home area is very restrictive as compared
to the limitation on memory used.
<p>
Storage restrictions, in particular inode restrictions, seem somewhat 
more restrictive than is necessary.
<p>
inode limits, especially in temp seem weird.  also, what exactly do the 
previously two questions mean?? 
<p>
In general, the inode limitations on temporary and home disk space are very
restrictive.  Having limits on the number of inodes used seems excessively
limiting.  Good configuration of the drive space shouldn't require these type
limitations. [...]

<p>
<li><b><a name=memory>Comments on memory</a>: &nbsp; 2 responses</b>
<p>
I like that it has 256 memory .. the nersc t3e has been the only machine
I have access to with this kind of memory .. I need that for part of my
work. 
<p>
[...] The machine need more memory per PE, the current 256MB severely limit
what I can do on it. I'd need 0.5-1GB/PE to get  larger calculations done. 

<p>
<li><b><a name=othert3e>Other comments</a>: &nbsp; 5 responses</b>
<p>
[...] Would like access to more performance tuning on single
node performance (e.g. a quick reference on best set of compile flags for
good performance). The Cray documentation is either too verbose or do not
give any information at all. 
<p>
mcurie has been up and down a lot lately, but mostly it is pretty reliable.
It is much better than when new schedulers were being tested.  [...]
<p>
The queue stats are not very useful.  A useful stat would be something that
better informs which queue is best to run on "now," based on history, like a
year-long plot of daily Mflops/wait time, perhaps on a per-queue basis or
otherwise so we know if it's better to wait or try another queue. 
<p>
PVM calls cause my code to stop with errors very often.  Resubmits without 
changes are successful.
<p>
[...] Couldn't use  many
more than 16 PEs due to IO bottlenecks (so far, NERSC is working on
this).[...] 

<p>
<li><b><a name=dkt3e>Don't use it (yet) / no specific comments</a>: 
     &nbsp; 7 responses</b>
<p>
N/A
<p>
Ever so slowly I creep up on porting my major code from SMP architectures to 
MPP... 
<p>
I don't use it. 
<p>
Haven't used it yet 
<p>
I have not used it yet, but will start in FY00. 
<p>
(I haven't fully tested my code, but I plan to try it on a larger number 
of PEs than I have so far.) 
<p>
(This is based on the experience of my group, not personal experience.)
</ul>

<hr noshade>
<h4>Did the C90 retirement go as smoothly as possible? - 42 responses</h4>
<ul>
<li><b><a name=yes>Yes</a>: </a>: &nbsp; 31 responses</b>
<p>
I did not have any problems and found the website and consultants very 
helpful in storing and/or moving files around to accommodate the retirement.
<p>
Yes I was well prepared for it thanks to your encouragement to get on to the 
J90s
<p>
The transition from the C90 was handled fine. 
<p>
Yes, not bad. 
<p>
it was reasonably smooth for me 
<p>
Yes - I had moved essentially everything off the C90 
<p>
It was fine for me. 
<p>
No problems
<p>
No problems for me.
<p>
yes [22 responses]

<p>
<li><b><a name=yesbut>Yes, but...</a>: </a>: &nbsp; 2 responses</b>
<p>
Yes, it did.  It would have been nice to access files right away after the 
transition rather than waiting until after Jan. 20. 
<p>
Essentially, yes.  There were too many changes going on all at the same time 
back then, so it was overwhelming to us here.

<p>
<li><b><a name=no>No</a>:  &nbsp; 5 responses</b>
<p>
There was a lot on congestion on the J90 for a number of months following
this.  It didn't really seem like enough capacity was added to handle all the
users coming from the J90. 
<p>
C90 retirement meant that we lost the very fast f77 compiler and DISSPLA.
This has forced me to use the very slow F90 compiler on the J90s and much
code conversion to retrofit NCAR graphics. Also, the J90s are slower than the
C90 and could stand to have more interactive access. 
<p>
Wished the c90 was not shut down until full PVP cluster was in place. 
J90 interim had a very adverse effect on our productivity 
<p>
The C90 was a great machine.  "As smoothly as possible" should have meant 
keeping it on the floor.
<p>
No.

<p>
<li><b><a name=nouse>Didn't use the C90 / hadn't used it recently</a>:  
    &nbsp; 4 responses</b>
<p>
I did not use C90. 
<p>
I haven't used the C90 for a long time.  I found the tern around time on the 
J90's to be superior to the C90.
<p>
I never used it.   
<p>
It was perfectly smooth for me. I had nothing on the C90. 
</ul>

<hr noshade>
<h4>Does the current NERSC PVP cluster meet your vector computing needs?
      - 52 responses</h4>
<ul>
<li><b><a name=nopvp>No</a>:  &nbsp; 24 responses</b>
<p>
no. I have to wait long in batch queues 
<p>
NO!  The batch queuing environment results in wait times (for me) that are 
HUGE compared to what I was used to on the C90!  
<p>
The PVP cluster has been very difficult to use.  In FY99 there were many 
times that jobs I submitted took many days to start.   
<p>
Not really. Need higher performance. 
<p>
No-no-no!!! if the waiting time to run batch jobs is any indication.  I hope
the new queue system is a big improvement because the present situation is
nearly intolerable. 
<p>
The jobs seem to be waiting so long in the queue that I am at present 
discouraged to run jobs. But I do need this facility for my computing. 
<p>
The system seems to be overloaded given the long wait time on the batch
queue. More batch machines might help (of course, that requires money). I have
heard of a proposal to turn one of the batch machines into an interactive
machine. Since there aren't enough batch machines as it is, I am opposed to
any reduction in the batch resources. 
<p>
No, it is way too slow. I work on many systems all at the same time, and
the CRAYs are by far the slowest to get out my results. And I mean by one
or more orders of magnitude. However, I am not exploiting the vector
capabilities so perhaps this is not fair. 
<p>
Very slow-- lots of wall clock time. 
<p>
batch queues too slow<br> need more capacity 
<p>
turnaround slow in recent months, but I recognize the emphasis must be given 
to MPP, in regard to resource acquisition. 
<p>
Based on the very long wait time in the batch queues (sometimes as much
as a week or more) there seems to be greatly too little PVP resources. I
also notive very large numbers of jobs submitted by one or a few users. I
hope the batch system accounts for this and runs only a few of those jobs at
a time, otherwise, it is unfair and possibly wasteful, to encourage
submission of many jobs. 
<p>
turn-around for large memory jobs is tooslow 
<p>
Batch is too slow. 
<p>
No. they are very slow. 
<p>
I do not use it much because my jobs stay in queue for a very long time (It
might be related to the end of the year). 
<p>
Noty exactly, needs to wait long time to obtain the result, sometimes the
compiler (CC) fails to overcome any overflow proplems. Also, need the prompt
to be customized such that the user can retrieve the previous commands using
the arrow keys. 
<p>
NO. MORE processors needed.SV1 processors are NOT as fast as they are
supposed to be. Account charging makes it even worse. I have no clue why
I am charged 3 times more on SV1 than I was on J90SEs, my code runs only
2 times faster! SV1 charging factor should be reduced to 1 or 1.2 . What
is the benefit of generously charging to users for NERSC? Total
allocation awards for FY2000 on SV1s is less than the total computing
power on the cluster. Repositories going over the allocation should be
automatically borrow time from the DOE reserve, as the process takes time
that users don't want to lose. It is a good strategy to urge users use
their allocations early in the year and take off some of it if they do not. 
<p>
No. So far, I have not managed to get better than about 180 Mflops out of
a single SV1 CPU. This compares with 500 Mflops for the C90 and 100
Mflops for the J90. I really need a vector machine with the C90's
capabilities.
<br> Multitasking on the J90/SV1 has problems of frequent
crashes and sometimes wrong results. Tracking down these problems is
almost impossible since there is no way of doing safe multiprocessing
i.e.multiprocessing which gives reproducible results down to the last
bit!
<br> The disk system is poorly configured, with insufficient inodes. It
needs to have its file systems rebuilt with of order 1 inode/5Kbytes
which is a typical value for workstations/PC's. The old 15000 inode quota
was totally inadequate: the new 5000 inode quota is totally
ridiculous. Adding a few more disks to the cluster would help. After all,
disks are dirt cheap.
<p>
Not a true replacement for the C90.  Switching one or two of the SV1's to
interactive use would help.  They'll be a small part of the total resource,
soon enough. 
<p>
I do interactive software development and testing now and then on
killeen.  Sometimes interactive response time is slow in the afternoons.
I think the SV1's are currently "batch only."  How about giving
interactive access to one of them? 
<p>
No. There should be one more box (SV1) available for interactive use.
<p>
NO...NOT AT ALL. THIS IS VERY IMPORTANT TO ME AND MY GROUP. We need to have
at least 1 of the three machines devoted to pure interactive use. I recommand
2 of 3 with interactive in the day time and 3 of 3 batch during 11-5 PDT. The
J90 is such a dog. Unless the PVP cluster gets significant interactive use it
is likely we will not find it very useful and will increasingly turn to other
computing resources. <br><p>
<p>
The interactive J90 is very slow.  Batch access of late has been terrible.

<p>
<li><b><a name=probno>Probably no / am waiting to see</a>: &nbsp; 
    3 responses</b>
<p>
The configuration is OK, but the interactive machine is often too busy. A
small persistent annoyance is the long wait to get job status info out of
cqstatl.  My main problem is my network connection - I have had
consulting help a couple of times on this, and it has always ended up with
you saying it's <br> the fault of the CUNY network provider and them saying
the delays are on the NERSC side.   
<p>
Since this is a fairly new cluster - since the SV1 is a new machine - will 
hold my fire. 
<p>
Have not used it. Can I run BASIS codes there? I doubt it. 

<p>
<li><b><a name=yespvp>Yes</a>: </a>: &nbsp; 21 responses</b>
<p>
It is ok.
<p>
My vector computing needs are very small. 
<p>
NA.  I only run short interactive jobs. 
<p>
No complaints 
<p>
NO problems so far, but I haven't made any significant demands lately.
<p>
yes [16 responses]

<p>
<li><b><a name=yesbutpvp>Yes, but... / yes and no</a>: &nbsp; 3 responses</b>
<p>
Yes, but with the complaint given below that the interactive response of the 
J90se is often too slow.
<p>
yes, but note that the compilation is done on a J90 while the software is
run on an SV-1. Doesn't this slow the code since it is not compiled on
the SV-1? 
<p>
Yes and no. While it is good to have the faster SV1s available, I don't use
the machines as much as I could simply because interactive response on the
J90 is very poor. Simple UNIX operations such as 'cd' and 'ls' can takes many
seconds to happen. Also, I often use my codes in an interactive manner,
mixing significant computation with interactive steering. While it would be
possible to work this way using batch (i.e. submitting little batch jobs
between each bit of steering), the current state of the batch queues
(overloaded) makes that completely impractical. (Note that I do use batch
when interactive steering is not needed.)<br> I strongly urge you to make the
interactive machine a SV1. This would ease up the overload on the K machine
and make it much easier for us to do our work. It is inevitable that anyone
who makes use of the batch machines will also make use of the interactive
machine, for testing and debugging purposes. Putting that on the slowest
machine available impedes our work. Also, making the

<p>
<li><b><a name=napvp>No answer</a>: &nbsp; 1 response</b>
<p>
N/A 
</ul>

<hr noshade>
<h4>Comments on NERSC's Cray PVP Cluster - 26 responses</h4>

<ul>
<li><b><a name=slow>Slow throughput, slow processors, need more PVP 
    cycles</a>: &nbsp; 15 responses</b>
<p>
The batch response is often slow, presumably due to high demand.  The 36
hour estimate above [for batch job wait time] averages 24 over the summer 
and 48 lately. 
<p>
Over the last 5 years have been solely using the scalar/vector machines 
at NERSC. In the last 2-3 years, shifted to the PVP cluster. In the early
stages stability of machines were very questionable - memory and disk
failures + communication between machines (loss of output). Bascially
could not run large jobs (memory / disk / time) on this cluster. Reduced, at
least for the past several years, to run simple short to medium runs on the
PVP machines. The point of NERSC was supply users to state-of-the-art
computing resources and thus push the limits of these machines. The PVP
cluster has failed to live up to the promise both in terms of software (OS)
and hardware failures. Maybe the SV1 will overcome these problems - see
answer to the above [since the SV1 is a new machine -
will hold my fire]. . 
<p>
[...] looooong wait in batch queues 
<p>
[...] The reconfiguring of the batch queues so that the
"nice" value decides when the job starts, rather than its priority during
execution was the stupidest idea to come out of NERSC since the decision
to replace the C90 with the J90/SV1's. The ideal is a system where the
nice value acts merely in the way it is supposed to under the Unix
standard, and the user is given the capability to change the nice value
up and down over the whole range allowed for the batch queues (say 5-19).
This would be similar to the old control command, but without separate
slow, medium and fast queues. As it stands, I sometimes have to wait
almost a week to get a job to start. I am tempted to submit multiple
copies of each job with different nice values.
<p>
It's usable but most people I speak with feel it should be avoided unless
absolutely necessary. Mostly because the performance is so poor. [...]
<p>
So far, the batch wait times hve been longer under the charging system
started Oct. 1 than they were last year under the old system. 
<p>
See above. [MORE processors needed.SV1 processors are NOT as fast as they are
supposed to be. Account charging makes it even worse...]
<p>
Currently, the wait time before jobs begin is completely unacceptable. I have
to wait 2-3 days for a job to begin. The date today is Sept. 28, 1999.
Hopefully, after Oct. 1 when the fiscal year begins anew, the wait time will
be less. 
<p>
These computers are not very fast.  I learned parallel computing techniques
to get away from using them.  I now use them only for diagnostic
post-processors that require NCAR graphics, which is not supported on MCURIE.
<p>
I have found the time waiting in the queue extremely long!!!! 
<p>
[...] but wait long time to get the my share of cpu time. 
<p>
There was a big backlog of jobs during the summer.  I haven't run any 
jobs recently so am not aware of how things stand at present.  
<p>
My only complaint is that in the last few months the turnaround time to
perform my simulations has gone way up.  When I first started using NERSC
computers (circa 1995), it took about a day to cycle a moderate simulation
through the 5MW queue.  Now, I have had jobs sitting on the queue for several
days before they even begin. 
<p>
Get more SV1 machines if batch remains over-subscribed. 
<p>
I think that my answer to the previous question says most of it [low megaflops,
...] Don't
try to argue that I am an isolated case since most of your users seem
satisfied. Note that a number of your big users have moved off the PVP
cluster entirely.

<p>
<li><b><a name=needint>Need interactive SV1s, interactive too slow</a>:
    5 responses</b>
<p>
slow interactive response; [...]
<p>
could use an interactive PVP machine 
<p>
I think you should seriously consider making one of the SV1's the interactive
machine rather than the J90se. Many users were expecting that this would be
the case before the SV1's arrived.  They were disappointed then when there
was no improvement in interactive use.  There still seem to be many times
when the J90se has very slow interactive response.  Since this uses up
valuable human time staring at a monitor waiting for something to happen it
is worth trying to improve. 
<p>
[...] only use
killeen, wish it was faster; how about making one of the faster machines
available for interactive use?
<p>
Maybe more interactive sessions on machines other than killeen. 

<p>
<li><b><a name=pvpsoft>Comments on software, configuration</a>: &nbsp; 
    4 responses</b>
<p>
[...] I also
don't like the non-standard aspects of the way they are set up. Most
annoying is the automatic logout on killeen - that's just ridiculous and
everyone knows how to circumvent it anyway. 
<p>
Make the F90 compiler as fast as F77 was on the C90. inept f90 compiler;
lacks graphics software that was on c90 
<p>
I think that my answer to the previous question says most of it [...,
multitasking instabilities, disk system poorly configured, insufficient
inodes]. 
<p>
[...] Need more information about text editors.

<p>
<li><b><a name=pvpgood>They're good machines, satisfied</a>: &nbsp; 
      4 responses</b>
<p>
My needs are simple. I only run short interactive post-processing on killeen
<p>
Very stable and good software development/testing environment. [...]
<p>
it's great 
<p>
Excellent accuracy, but wait long time [...]

<p>
<li><b><a name=pvpq>Comments on queues</a>: &nbsp; 3 responses</b>
<p>
Possibility of running  batch jobs with  RAM of 200 MW, Disk 50 Gb , and a
max of CPU about 100 -150 hrs at J cluster should be made available in
the near future. 
<p>
The batch queue system functions but has features that really seem
candidates for improvement. The queuing system seems unpredictable and
somewhat unreliable.  Jobs seem to be scheduled in random order.  Of two
similar jobs submitted simultaneously, one could run within 12 hours,
skipping over several hundred other jobs in the queue, the other could
wait many days.  So, it does little good to submit jobs in priority
order in order to do a systematic series of jobs where the next step 
depends on previous results. Some users seem to be able to submit 100
jobs and have them all run very quickly. Occasionally, jobs are "failed"
because of a system problem and it is necessary to resubmit and wait
again several days for the job to start after already having waited several
days before the system problem was encountered.  When a system problem
is causing jobs to fail, often jobs continue to be submitted, all of
which fail causing the problem to multiply.  Finally, it
<p>
As a beginner, I experienced frustration with exceeded CPU- and disk
space limits (due to a core file of one job). Is there a possibility to
automatically (or manually) keep alive or resubmit jobs that crash due to
such external problems? The introduction of premium jobs helps greatly to
detect errors in the batch script. 

<p>
<li><b><a name=pvpnot>Haven't used them</a>: &nbsp; 2 responses</b>
<p>
I have not used the Cray PVP cluster 
<p>
N/A 
</ul>

<hr noshade>
<h4>Comments on NERSC's HPSS Storage System - 38 responses</h4>

<ul>
<li><b><a name=likehpss>It's a good system, satisfied</a>:  
    &nbsp; 12 responses</b>
<p>
Very good job on maintaining such a monster storage system. 
<p>
HPSS is the best mass storage system I have used for the last 20 years. 
<p>
I have a lot of data stored.  When I need it, it is there, so I am happy. 
<p>
mostly works well [...]
<p>
Very nice and efficient system, a great way to store data. Can handle<
large files (8+GB) easily, which is extremely useful. 
<p>
Very good 
<p>
Without HPSS , I could not use NERSC computing facilities.It is a sine quo
non for my work.
<p>
I use the storage to HPSS thru my batch jobs since I use the stored files for
restarting my programs. Perfomance is not a big thing for me but reliability
of HPSS being up to store the data is. I am very satisfied with the
reliability. 
<p>
very useful, for backing up files from my work computer system 
<p>
It's good but [...]
<p>
It works for me. 
<p>
useful for our RNC group to storage the mass data. 

<p>
<li><b><a name=interface>Would like a better user interface</a>:  
     &nbsp; 11 responses</b>
<p>
It would be nice to have better interface software to HPSS. I'm thinking of
something that would let one see the whole (or large portions of) one's
directory tree structure in a graphical display.  Also, some type of user
friendly search capability would be a big help. 
<p>
It would be nice if the system were less like using ftp like, and more like 
using an NFS fileserver.
<p>
[...] (Please fix the HSI wildcard char recog.!  Seems sensitive to term 
emulation, backspace, mouse-paste etc.)
<p>
I would be nice to have masget etc. commands we used to have for the mass 
storage before. These commands might exist, and I am just not aware of them.
<p>
[...] ftp interface a bit primitive 
<p>
Again, because I'm lazy, I'l like the interface to look as much exactly
like Unix as possible.  I'd like to be able to use foreach loop and
wildcards in the same way that I do in tcsh...it's pretty close, but
still frustrating at times. 
<p>
The FTP interface is a bit clumsy for many problems, and it appears that
the FTP restart command is not supported, so there is no way to restart a
failed transfer, or just get a portion of a file.  
<p>
More UNIX like interface is an improvement. 
<p>
It's good but needs some super-simple wrappers to allow one to store away
(and retrieve) a whole directory in one fell swoop. I can write these for
myself, but really they should be provided. For example, a command called
"stash" that just tar's the files in the current directory (not below!)
and stuffs that into a like-named directory in HPSS would be a great
help. I wrote such a tool for CFS but dare not try using it now without
careful attention to the changes needed.
<p>
[...] HPSS needs to be configured to send an
error flag to the shell when the transfer fails so that a batch job could
be made to crash when a transfer failed. Note, cfs was able to do this.
<p>
It should be easier to search the whole directory structure for files by name, 
date, etc.

<p>
<li><b><a name=xfer>Improve the transfer rate</a>:  &nbsp; 7 responses</b>
<p>
It's not as fast as NCAR's MSS. [...]
<p>
Improve transfer rate 
<p>
I believe my upper bound on the transfer rate is due to the network, but I
which nevertheless that it could be higher. At Brookhaven I have (or used to
have) 10 MB/sec.
<p>
FTP is slow 
<p>
slow at transferring lots of large files; needs bigger pipe 
<p>
At times, the HPSS can be slow. 
<p>
Reliability and performance appear to be highly variable.  At times both are
good for long stretches.  At other times, they appear to degrade and remain
so for a considerable time. 

<p>
<li><b><a name=cmd>Command response too slow</a>:  &nbsp; 4 responses</b>
<p>
It might be useful if the ls command did not take so long when deep in a
directory structure. This is often the decision maker as to what files
are needed. 
<p>
HPSS is too slow in responding to directory commands in directories with
more than a few thousand files. [...]
<p>
System response seems slow, but I have no expertise by which to judge it
<p>
I'm not sure how hpss works, but there seems to be a lengthy delay after
entering some command like "dir" or "get file" before it begins
execution.  Probably this is unavoidable due to the size of the storage
system. 

<p>
<li><b><a name=reliability>Stability problems</a>:  &nbsp; 3 responses</b>
<p>
PFTP seems to die transferring large files more often that I would expect/hope.
<p>
The system does not seem to be very stable. 
<p>
Reliability and performance appear to be highly variable.  At times both are
good for long stretches.  At other times, they appear to degrade and remain
so for a considerable time. 

<p>
<li><b><a name=pswd>Password problems</a>:  &nbsp; 2 responses</b>
<p>
Right now it takes a long, long time to authenticate my password. 
<p>
sometimes it requires the password more than once 

<p>
<li><b><a name=dkhpss>Don't know what it is, don't use</a>:  
    &nbsp; 4 responses</b>
<p>
Don't use it as much as I should as I was a cfs user and have not taken much 
time to learn how to use the new system. 
<p>
I am told that I have an HPSS storage system account.  I haven't the faintest 
idea what that means.
<p>
I have not used the NERSC's HPSS storage system 
<p>
I'm not sure if this is an HPSS issue, per se, but I don't understand why
i-nodes are so limited on t3e machine.  In our work, we have lots of
small files, so this has been a problem.
</ul>

<hr noshade>
<h4>Comments about NERSC's auxiliary servers - 10 responses</h4>

<ul>
<li><b><a name=likeaux>Like them</a>:  &nbsp; 2 responses</b>
<p>
They are powerful, useful, and well maintained. 
<p>
Good response times. 

<p>
<li><b><a name=newton>Newton needs an upgrade</a>:  &nbsp; 1 response</b>
<p>
Newton needs to be upgraded with more powerful processor(s) 

<p>
<li><b><a name=newweb>Provide a data broadcasting web server</a>:  
      &nbsp; 1 response</b>
<p>
You might consider also providing a password protected web-server where users
could broadcast data from their runs on the PVP and MPP machines directly to
themselves and to their collaborators.  It would be particularly useful if
such a server could provide some software (e.g., using JAVA) which would
gather one's data off of PVP or MPP platforms and then make graphical
web-based displays of it.  I recently saw a demonstration of such a
collaborative graphical simulation environment developed at a Japanese
supercomputer center (at JAERI) and it looked like a very useful capability.

<p>
<li><b><a name=dkaux>Don't use them, no answer</a>:  &nbsp; 6 responses</b>
<p>
don't use these servers 
<p>
Not used 
<p>
No Answer 
<p>
Have not had the time or pressing need to use escher this past year. 
<p>
N/A 
<p>
I have not used them. 
</ul>

<hr noshade>
<P><A HREF="./software.html">Next</A>: Software Resources


<!-- The following "exec" server side include generates a
footer for this file.  You can control what is generated by
passing parameters to the script - e.g. your username as
"CONTACT".  Please see:

https://www.nersc.gov/staff/USG/webdocs/headers.html

for more information about how to control the footer script.
-->

</body>



